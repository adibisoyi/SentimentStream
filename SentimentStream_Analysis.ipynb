{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNNONKsQzswk",
        "outputId": "51158e5c-c7de-4792-e171-8d62b6367376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE4pKWRQz4ae",
        "outputId": "37ab6b95-39dc-4175-f35c-5993b892a8a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRsyDaMF2OQK",
        "outputId": "f9f083e5-92eb-48f4-ca13-bcb5ae060c45"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import requests\n",
        "import aiohttp\n",
        "import asyncio\n",
        "from transformers import pipeline\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import nest_asyncio\n",
        "\n",
        "# Patch asyncio to allow nested loops (for interactive environments like Jupyter)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Delete existing database if it exists to reset the table schema\n",
        "if os.path.exists('news_feed.db'):\n",
        "    os.remove('news_feed.db')\n",
        "    print(\"Old database removed. A new one will be created.\")\n",
        "\n",
        "# Set up database connection\n",
        "conn = sqlite3.connect('news_feed.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create table to store news articles with VADER and TextBlob sentiment\n",
        "cursor.execute('''CREATE TABLE IF NOT EXISTS news (\n",
        "                    title TEXT,\n",
        "                    summary TEXT,\n",
        "                    vader_sentiment REAL,\n",
        "                    textblob_sentiment REAL,\n",
        "                    url TEXT\n",
        "                )''')\n",
        "conn.commit()\n",
        "\n",
        "# API Key\n",
        "api_key = 'your API Key",
        "\n",
        "# Asynchronous function to fetch news articles\n",
        "async def fetch_news_async(session, query, api_key, page_size=10):\n",
        "    url = f'https://newsapi.org/v2/everything?q={query}&apiKey={api_key}&pageSize={page_size}'\n",
        "    async with session.get(url) as response:\n",
        "        if response.status != 200:\n",
        "            print(f\"Failed to fetch articles: {response.status}\")\n",
        "            return []\n",
        "        data = await response.json()\n",
        "        return data.get('articles', [])\n",
        "\n",
        "# Function to summarize articles\n",
        "def summarize_article(article_text):\n",
        "    summarizer = pipeline('summarization', model=\"facebook/bart-large-cnn\")\n",
        "    summary = summarizer(article_text, max_length=min(150, len(article_text)), min_length=40, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Function to analyze sentiment using both VADER and TextBlob for comparison\n",
        "def analyze_sentiment(article_text):\n",
        "    # VADER sentiment\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    vader_sentiment = analyzer.polarity_scores(article_text)['compound']\n",
        "\n",
        "    # TextBlob sentiment\n",
        "    blob = TextBlob(article_text)\n",
        "    textblob_sentiment = blob.sentiment.polarity\n",
        "\n",
        "    return vader_sentiment, textblob_sentiment\n",
        "\n",
        "# Asynchronous function to process and store articles\n",
        "async def process_articles(query, api_key):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        articles = await fetch_news_async(session, query, api_key)\n",
        "        news_feed = []\n",
        "\n",
        "        for article in articles:\n",
        "            title = article['title']\n",
        "            description = article.get('description', '')\n",
        "            content = article.get('content', '')\n",
        "            full_text = f\"{title} {description} {content}\"\n",
        "\n",
        "            # Summarize and analyze sentiment\n",
        "            summary = summarize_article(full_text)\n",
        "            vader_sentiment, textblob_sentiment = analyze_sentiment(summary)\n",
        "\n",
        "            # Append to list and store in database\n",
        "            news_feed.append({\n",
        "                'title': title,\n",
        "                'summary': summary,\n",
        "                'vader_sentiment': vader_sentiment,\n",
        "                'textblob_sentiment': textblob_sentiment,\n",
        "                'url': article['url']\n",
        "            })\n",
        "            cursor.execute(\"INSERT INTO news (title, summary, vader_sentiment, textblob_sentiment, url) VALUES (?, ?, ?, ?, ?)\",\n",
        "                           (title, summary, vader_sentiment, textblob_sentiment, article['url']))\n",
        "\n",
        "        # Commit changes to the database\n",
        "        conn.commit()\n",
        "\n",
        "        # Convert to DataFrame for easy visualization\n",
        "        df = pd.DataFrame(news_feed)\n",
        "\n",
        "        # Save DataFrame to an Excel file\n",
        "        df.to_excel(\"news_feed.xlsx\", index=False)\n",
        "        print(\"News feed saved to 'news_feed.xlsx'\")\n",
        "\n",
        "        return df\n",
        "\n",
        "# Function to retrieve news from the database\n",
        "def get_saved_news():\n",
        "    cursor.execute(\"SELECT * FROM news\")\n",
        "    rows = cursor.fetchall()\n",
        "    df = pd.DataFrame(rows, columns=['Title', 'Summary', 'VADER Sentiment', 'TextBlob Sentiment', 'URL'])\n",
        "\n",
        "    # Save retrieved data to Excel as well\n",
        "    df.to_excel(\"saved_news_feed.xlsx\", index=False)\n",
        "    print(\"Saved news feed exported to 'saved_news_feed.xlsx'\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Run the process_articles asynchronously in a Jupyter-compatible way\n",
        "async def get_news_feed(query):\n",
        "    return await process_articles(query, api_key)\n",
        "\n",
        "# Example usage\n",
        "query = \"artificial intelligence\"\n",
        "news_feed_df = asyncio.run(get_news_feed(query))\n",
        "\n",
        "# Display the latest news feed with sentiment comparison\n",
        "print(\"Latest News Feed with Sentiment Comparison:\")\n",
        "print(news_feed_df[['title', 'summary', 'vader_sentiment', 'textblob_sentiment', 'url']])\n",
        "\n",
        "# Fetch and display saved news with sentiment comparison\n",
        "saved_news_df = get_saved_news()\n",
        "print(\"\\nSaved News Feed with Sentiment Comparison:\")\n",
        "print(saved_news_df)\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DbOuW0N2Ef-",
        "outputId": "72442255-54c7-4c19-eadc-9bc27074c70e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old database removed. A new one will be created.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Your max_length is set to 150, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n",
            "Your max_length is set to 150, but your input_length is only 86. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
            "Your max_length is set to 150, but your input_length is only 112. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=56)\n",
            "Your max_length is set to 150, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
            "Your min_length=40 must be inferior than your max_length=29.\n",
            "Your max_length is set to 29, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1282: UserWarning: Unfeasible length constraints: `min_length` (40) is larger than the maximum possible length (29). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
            "  warnings.warn(\n",
            "Your max_length is set to 150, but your input_length is only 96. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
            "Your max_length is set to 150, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
            "Your max_length is set to 150, but your input_length is only 92. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n",
            "Your max_length is set to 150, but your input_length is only 99. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n",
            "Your max_length is set to 150, but your input_length is only 96. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "News feed saved to 'news_feed.xlsx'\n",
            "Latest News Feed with Sentiment Comparison:\n",
            "                                               title                                            summary  vader_sentiment  textblob_sentiment                                                url\n",
            "0         How a Trump Win Could Unleash Dangerous AI  Trump's opposition to “woke” safety standards ...           0.4215           -0.184517  https://www.wired.com/story/donald-trump-ai-sa...\n",
            "1  This Toilet Seat Has a Camera for Taking Pictu...  This Toilet Seat Has a Camera for Taking Pictu...           0.8074           -0.211111  https://gizmodo.com/this-toilet-seat-has-a-cam...\n",
            "2  President Biden sets up new AI guardrails for ...  President Biden sets up new AI guardrails for ...           0.8750           -0.044805  https://consent.yahoo.com/v2/collectConsent?se...\n",
            "3  Nobel Prize Goes to ‘Godfathers of AI’ Who Now...  Two AI researchers, John Hopfield and Geoffrey...           0.5574           -0.150000  https://gizmodo.com/nobel-prize-goes-to-godfat...\n",
            "4                                          [Removed]  CNN.com will feature iReporter photos in a wee...           0.0000            0.000000                                https://removed.com\n",
            "5  Here's how Anthropic CEO Dario Amodei defines ...  Anthropic CEO Dario Amodei weighs in on where ...           0.5845           -0.028409  https://www.businessinsider.com/how-anthropic-...\n",
            "6  I Used AI to Help With Grief. Here's What I Le...  I Used AI to Help With Grief. Here's What I Le...          -0.0507           -0.153125  https://www.cnet.com/tech/services-and-softwar...\n",
            "7  This is OpenAI CEO Sam Altman's favorite quest...  OpenAI CEO Sam Altman says AGI will facilitate...           0.0000            0.500000  https://www.businessinsider.com/openai-ceo-sam...\n",
            "8           How to Use AI to Save Money on Groceries  Food inflation was at a 43-year high in August...           0.8074            0.186667  https://www.cnet.com/tech/services-and-softwar...\n",
            "9  Reddit CEO says the platform is in an 'arms ra...  Reddit CEO says the platform is in an 'arms ra...           0.8074            0.200000  https://www.businessinsider.com/reddit-ceo-pla...\n",
            "Saved news feed exported to 'saved_news_feed.xlsx'\n",
            "\n",
            "Saved News Feed with Sentiment Comparison:\n",
            "                                               Title                                            Summary  VADER Sentiment  TextBlob Sentiment                                                URL\n",
            "0         How a Trump Win Could Unleash Dangerous AI  Trump's opposition to “woke” safety standards ...           0.4215           -0.184517  https://www.wired.com/story/donald-trump-ai-sa...\n",
            "1  This Toilet Seat Has a Camera for Taking Pictu...  This Toilet Seat Has a Camera for Taking Pictu...           0.8074           -0.211111  https://gizmodo.com/this-toilet-seat-has-a-cam...\n",
            "2  President Biden sets up new AI guardrails for ...  President Biden sets up new AI guardrails for ...           0.8750           -0.044805  https://consent.yahoo.com/v2/collectConsent?se...\n",
            "3  Nobel Prize Goes to ‘Godfathers of AI’ Who Now...  Two AI researchers, John Hopfield and Geoffrey...           0.5574           -0.150000  https://gizmodo.com/nobel-prize-goes-to-godfat...\n",
            "4                                          [Removed]  CNN.com will feature iReporter photos in a wee...           0.0000            0.000000                                https://removed.com\n",
            "5  Here's how Anthropic CEO Dario Amodei defines ...  Anthropic CEO Dario Amodei weighs in on where ...           0.5845           -0.028409  https://www.businessinsider.com/how-anthropic-...\n",
            "6  I Used AI to Help With Grief. Here's What I Le...  I Used AI to Help With Grief. Here's What I Le...          -0.0507           -0.153125  https://www.cnet.com/tech/services-and-softwar...\n",
            "7  This is OpenAI CEO Sam Altman's favorite quest...  OpenAI CEO Sam Altman says AGI will facilitate...           0.0000            0.500000  https://www.businessinsider.com/openai-ceo-sam...\n",
            "8           How to Use AI to Save Money on Groceries  Food inflation was at a 43-year high in August...           0.8074            0.186667  https://www.cnet.com/tech/services-and-softwar...\n",
            "9  Reddit CEO says the platform is in an 'arms ra...  Reddit CEO says the platform is in an 'arms ra...           0.8074            0.200000  https://www.businessinsider.com/reddit-ceo-pla...\n"
          ]
        }
      ]
    }
  ]
}
